{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and auxiliar functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas\n",
    "# !pip install scikit-learn\n",
    "# !pip install statsmodels\n",
    "# !pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit,GridSearchCV\n",
    "\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "from sklearn.linear_model import Lasso, Ridge, ElasticNet\n",
    "from sklearn.ensemble import BaggingRegressor, GradientBoostingRegressor, RandomForestRegressor\n",
    "\n",
    "from sklearn.metrics import root_mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "\n",
    "\n",
    "# Setting a random state\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "\n",
    "# Create lagged features\n",
    "def create_lagged_columns(df, list_cols, list_lags, df_with_target=None):\n",
    "    \"\"\"Create lagged covariates for time series forecasting.\"\"\"\n",
    "    lagged_data = {}\n",
    "    for column in list_cols:\n",
    "        for lag in list_lags:\n",
    "            if df_with_target is None:\n",
    "                lagged_data[f\"{column}_lag{lag}\"] = df[column].shift(lag)\n",
    "            else:\n",
    "                lagged_data[f\"{column}_lag{lag}\"] = df_with_target[column].shift(lag)\n",
    "\n",
    "    lagged_df = pd.DataFrame(data=lagged_data, index=df.index)\n",
    "\n",
    "    return lagged_df\n",
    "\n",
    "\n",
    "\n",
    "def walk_foward_validation(model, param_grid, X_train, y_train):\n",
    "    tscv = TimeSeriesSplit(n_splits=2, test_size=1)  # changed from n_splits=12 to n_splits=2 due to time complexity\n",
    "\n",
    "    # GridSearchCV to find the best hyperparameters using Walk-Forward CV\n",
    "    search = GridSearchCV(model, param_grid, cv=tscv, scoring='neg_root_mean_squared_error')\n",
    "\n",
    "    # Perform the grid search over the training set\n",
    "    search.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "    # Output the best parameters from the grid search\n",
    "    print(f\"Best Estimator: {search.best_estimator_}\")\n",
    "\n",
    "    return search.best_estimator_\n",
    "\n",
    "\n",
    "\n",
    "def recursive_forecast(model, y_train, X_train, X_test, forecast_horizon, target_variable, lags_target, cv=False):\n",
    "    predictions = []\n",
    "    y_train_step = y_train.copy()\n",
    "\n",
    "    if X_test is None:\n",
    "        for i in range(forecast_horizon):\n",
    "            model_func = model(y_train=y_train_step)\n",
    "            model_fitted = model_func.fit()\n",
    "\n",
    "            # Predict the next step\n",
    "            y_pred = model_fitted.predict(start=len(y_train_step), end=len(y_train_step))\n",
    "            predictions.append(y_pred.iloc[0])\n",
    "\n",
    "            new_row_prediction = y_pred.to_frame(name=y_train_step.columns[0])\n",
    "            y_train_step = pd.concat([y_train_step, new_row_prediction])\n",
    "\n",
    "    else:\n",
    "        model_func = model(X_train, y_train_step)\n",
    "        model_fitted = model_func.fit(X=X_train, y=y_train.values.ravel())\n",
    "\n",
    "        for i in range(forecast_horizon):\n",
    "            X_test_step = X_test.iloc[i:i+1]  # one step ahead forecast\n",
    "\n",
    "            lag_target_columns = [f\"{target_variable}_lag{lag}\" for lag in lags_target]\n",
    "            lag_target_values = y_train_step.iloc[[-lag for lag in lags_target]].values.flatten().reshape(1,-1)\n",
    "            X_test_lag = pd.DataFrame(data=lag_target_values, columns=lag_target_columns, index=X_test_step.index)\n",
    "\n",
    "            X_test_for_pred = pd.concat([X_test_step, X_test_lag], axis=1)\n",
    "\n",
    "            # Predict the next step\n",
    "            y_pred = model_fitted.predict(X=X_test_for_pred)[0]\n",
    "            predictions.append(y_pred)\n",
    "\n",
    "            # Append the new row with the prediction to the training data\n",
    "            new_row_prediction = pd.DataFrame({target_variable: [y_pred]}, index=X_test_step.index)\n",
    "            y_train_step = pd.concat([y_train_step, new_row_prediction])\n",
    "\n",
    "    return np.array(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Random Walk (RW)\n",
    "def random_walk(y_train, forecast_horizon):\n",
    "    last_observation = y_train.iloc[-1].values[0]\n",
    "    return np.array([last_observation] * forecast_horizon)\n",
    "\n",
    "# 2. Autoregressive (AR)\n",
    "def autoregressive(y_train):\n",
    "    y_train = y_train.asfreq('MS')\n",
    "    model = AutoReg(y_train, lags=12)\n",
    "    return model\n",
    "\n",
    "# 3. LASSO Regression\n",
    "def lasso(X_train, y_train):\n",
    "    model = Lasso(max_iter=10000)\n",
    "    param_grid = {\n",
    "        'alpha': [0.0001, 0.0003, 0.0006, 0.001, 0.003, 0.006, 0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6]\n",
    "    }\n",
    "    best_model = walk_foward_validation(model=model, param_grid=param_grid, X_train=X_train, y_train=y_train)\n",
    "    return best_model\n",
    "\n",
    "# 4. Ridge Regression\n",
    "def ridge(X_train, y_train):\n",
    "    model = Ridge(max_iter=10000)\n",
    "    param_grid = {\n",
    "        'alpha': [0.1, 0.3, 0.6, 1, 3, 6, 10, 13, 16, 20, 30, 40, 50, 60, 70, 80, 90, 100, 150, 200, 300]\n",
    "    }\n",
    "    best_model = walk_foward_validation(model=model, param_grid=param_grid, X_train=X_train, y_train=y_train)\n",
    "    return best_model\n",
    "\n",
    "# 5. Elastic Net (Elnet)\n",
    "def elastic_net(X_train, y_train):\n",
    "    model = ElasticNet(max_iter=40000)\n",
    "    param_grid = {\n",
    "        'alpha': np.arange(0.01, 0.52, 0.05),\n",
    "        'l1_ratio': np.arange(0.01, 0.52, 0.05)\n",
    "    }\n",
    "    best_model = walk_foward_validation(model=model, param_grid=param_grid, X_train=X_train, y_train=y_train)\n",
    "    return best_model\n",
    "\n",
    "# 6. Bagging (Bootstrap Aggregating)\n",
    "def bagging(X_train, y_train):\n",
    "    model = BaggingRegressor()\n",
    "    param_grid = {\n",
    "        'n_estimators': np.arange(150, 350, 50),            # Number of base models in ensemble\n",
    "        'max_features': np.arange(0.1, 1, 0.1),             # Proportion of features used to train each base model\n",
    "        'bootstrap': [False],                               # Do not use bootstrap samples\n",
    "        'bootstrap_features': [True],                       # Use bootstrap features\n",
    "    }\n",
    "    best_model = walk_foward_validation(model=model, param_grid=param_grid, X_train=X_train, y_train=y_train)\n",
    "    return best_model\n",
    "\n",
    "# 7. Gradient Boosting\n",
    "def gradient_boosting(X_train, y_train):\n",
    "    model = GradientBoostingRegressor()\n",
    "    param_grid = {\n",
    "        'n_estimators': np.arange(200, 400, 50),            # Number of boosting stages\n",
    "        'learning_rate': np.arange(0.01, 0.2, 0.05),        # Learning rate between 0.01 and 0.2\n",
    "        'max_depth': np.arange(3, 20, 2),                   # Maximum depth of trees\n",
    "        'min_samples_split': np.arange(6, 20, 2),           # Minimum number of samples to split a node\n",
    "        'min_samples_leaf': np.arange(1, 4, 1),             # Minimum number of samples at a leaf node\n",
    "        'subsample': np.arange(0.8, 1.0, 0.05)              # Fraction of samples used for fitting each tree\n",
    "    }\n",
    "    best_model = walk_foward_validation(model=model, param_grid=param_grid, X_train=X_train, y_train=y_train)\n",
    "    return best_model\n",
    "\n",
    "# 8. Random Forest\n",
    "def random_forest(X_train, y_train):\n",
    "    model = RandomForestRegressor()\n",
    "    param_grid = {\n",
    "        'n_estimators': np.arange(100, 300, 50),            # Number of trees\n",
    "        'max_features': np.arange(0.3, 1.0, 0.05),          # Maximum features used in splits\n",
    "        'bootstrap': [False],                               # Do not use bootstrap samples\n",
    "    }\n",
    "    best_model = walk_foward_validation(model=model, param_grid=param_grid, X_train=X_train, y_train=y_train)\n",
    "    return best_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def bronze_to_silver(df):\n",
    "    # Ensure that modifications on one DataFrame do not affect the other\n",
    "    df = df.copy()\n",
    "\n",
    "    # Drop the first row, it was a dummy one\n",
    "    df = df.iloc[1:]\n",
    "\n",
    "    # Set the column 'date' as the index of the DataFrame\n",
    "    df = df.set_index('date').sort_index()\n",
    "\n",
    "    # Convert 'date' index to datetime if it's not already\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    \n",
    "    # Filter rows based on date range\n",
    "    df = df[(df.index >= '2000-01-01') & (df.index <= '2023-12-01')]\n",
    "\n",
    "    # Replace entries containing any of the keywords with NaN and remove columns with all entries as NaN\n",
    "    keywords = ['european', 'euro', 'germany', 'uk', ':']\n",
    "    for column in df.columns:\n",
    "        df[column] = df[column].apply(lambda x: np.nan if isinstance(x, str) and any(keyword in x.lower() for keyword in keywords) else x)\n",
    "        if df[column].isnull().all():\n",
    "            df.drop(column, axis=1, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def silver_to_gold(df):\n",
    "    # Ensure that modifications on one DataFrame do not affect the other\n",
    "    df = df.copy()\n",
    "\n",
    "    # Drop columns with all NaNs\n",
    "    df = df.dropna(axis=1)\n",
    "\n",
    "    # for each variable, keep the column with the highest degree of adjustment\n",
    "    # after droping na's to unsure that, in the end, atleast one viable variable is available\n",
    "    adjustment_degree = {\"SCA\": 1, \"SA\": 2, \"CA\": 3, \"NSA\": 4, \"NA\": 5}\n",
    "    dict_cols = {}\n",
    "    final_cols = []\n",
    "\n",
    "    for column in df.columns:\n",
    "        if column.startswith(\"HICP\"):\n",
    "            if not column.startswith(\"HICP_All_\"):  # HICP_All_idx, HICP_All_mor, HICP_All_anr\n",
    "                continue\n",
    "        parts = column.rsplit('_', 1)\n",
    "        key = parts[0]\n",
    "        suffix = parts[1]\n",
    "        value = (suffix, column)\n",
    "        \n",
    "        if key not in dict_cols:\n",
    "            dict_cols[key] = value\n",
    "        else:\n",
    "            current_suffix = dict_cols[key][0]\n",
    "            if suffix not in adjustment_degree:\n",
    "                print(f\"the suffix '{suffix}' is not a valid adjustment\")\n",
    "                continue\n",
    "            if adjustment_degree[suffix] < adjustment_degree[current_suffix]:\n",
    "                dict_cols[key] = value\n",
    "\n",
    "    for key, value in dict_cols.items():\n",
    "        final_cols.append(value[1])\n",
    "    \n",
    "    # Filter columns to include only the selected ones\n",
    "    df = df[final_cols]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def gold_to_model_data(df, target_variable, list_target_variables, test_size, list_lags_covariates, list_lags_target):\n",
    "    # Split the data into X_train, X_test, y_train, y_test by the target variable and the test size and standardize it\n",
    "\n",
    "    # Splitting target variable 'y'\n",
    "    # y is either \"HICP_All_idx_NA\", \"HICP_All_mor_NA\", \"HICP_All_anr_NA\"\n",
    "    y = df[[target_variable]]\n",
    "    y_train = y[:-test_size]\n",
    "    y_test = y[-test_size:]\n",
    "\n",
    "    # Standardizing 'y'\n",
    "    scaler_y = StandardScaler()\n",
    "    scaler_y_fitted = scaler_y.fit(y_train)\n",
    "    y_train_scaled = pd.DataFrame(scaler_y_fitted.transform(y_train), columns=y_train.columns, index=y_train.index)\n",
    "    y_test_scaled = pd.DataFrame(scaler_y_fitted.transform(y_test), columns=y_test.columns, index=y_test.index)\n",
    "\n",
    "    # Splitting feature set 'X'\n",
    "    X = df.drop(list_target_variables, axis=1)\n",
    "    X_train = X[:-test_size]\n",
    "    X_test = X[-test_size:]\n",
    "\n",
    "    # Standardizing 'X'\n",
    "    scaler_X = StandardScaler()\n",
    "    scaler_X_fitted = scaler_X.fit(X_train)\n",
    "    X_scaled = pd.DataFrame(scaler_X_fitted.transform(X), columns=X.columns, index=X.index)\n",
    "\n",
    "    # Creating lagged covariate features\n",
    "    X_lagged_covariates = create_lagged_columns(df=X_scaled, list_cols=X_scaled.columns, list_lags=list_lags_covariates)\n",
    "    X_scaled = pd.concat([X_scaled, X_lagged_covariates], axis=1)\n",
    "\n",
    "    # Creating lagged target features (appended only to the training set)\n",
    "    X_train_scaled = pd.DataFrame(X_scaled[:-test_size], columns=X_scaled.columns, index=X_train.index)\n",
    "    X_train_lagged_target = create_lagged_columns(df=X_train_scaled, list_cols=[target_variable], list_lags=list_lags_target, df_with_target=y_train_scaled)\n",
    "    X_train_scaled = pd.concat([X_train_scaled, X_train_lagged_target], axis=1)\n",
    "\n",
    "    # Preparing test set\n",
    "    X_test_scaled = pd.DataFrame(X_scaled[-test_size:], columns=X_scaled.columns, index=X_test.index)\n",
    "\n",
    "    # Aligning the training dataset\n",
    "    df_training_scaled_alignment = pd.concat([y_train_scaled, X_train_scaled], axis=1).dropna()\n",
    "    y_train_scaled_align = df_training_scaled_alignment[y_train_scaled.columns]\n",
    "    X_train_scaled_align = df_training_scaled_alignment[X_train_scaled.columns]\n",
    "\n",
    "    return X_train_scaled_align, X_test_scaled, y_train_scaled_align, y_test_scaled, scaler_X_fitted, scaler_y_fitted\n",
    "\n",
    "\n",
    "\n",
    "def model_data_to_model_evaluation(target_variable, model_func, X_train, X_test, y_train, y_test, scaler_X_fitted, scaler_y_fitted, forecast_horizon, list_lags_target):\n",
    "    evaluation = {}\n",
    "\n",
    "    if model_func.__name__ in ['random_walk']:\n",
    "        # Perform forecast for the horizon period\n",
    "        y_pred = model_func(y_train, forecast_horizon)\n",
    "    \n",
    "    elif model_func.__name__ in ['autoregressive']:\n",
    "        # Perform recursive forecast for the horizon period\n",
    "        y_pred = recursive_forecast(\n",
    "            model=model_func, y_train=y_train, X_train=None, X_test=None, forecast_horizon=forecast_horizon,\n",
    "            target_variable=target_variable, lags_target=list_lags_target\n",
    "        )\n",
    "\n",
    "    elif model_func.__name__ in ['lasso', 'ridge', 'elastic_net', 'bagging', 'gradient_boosting', 'random_forest']:\n",
    "        # Perform recursive forecast for the horizon period\n",
    "        y_pred = recursive_forecast(\n",
    "            model=model_func, y_train=y_train, X_train=X_train, X_test=X_test, forecast_horizon=forecast_horizon,\n",
    "            target_variable=target_variable, lags_target=list_lags_target\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        print(f\"Did not handle the model: {model_func.__name__}\")\n",
    "\n",
    "    y_test_for_horizon = y_test[:forecast_horizon]\n",
    "\n",
    "    y_test_inversed = scaler_y_fitted.inverse_transform(np.array(y_test_for_horizon).reshape(-1, 1))\n",
    "    y_pred_inversed = scaler_y_fitted.inverse_transform(np.array(y_pred).reshape(-1, 1))\n",
    "\n",
    "    # Evaluate the model\n",
    "    rmse = root_mean_squared_error(y_test_inversed, y_pred_inversed)  # Root Mean Squared Error\n",
    "    mae = mean_absolute_error(y_test_inversed, y_pred_inversed)  # Mean Absolute Error\n",
    "    mape = mean_absolute_percentage_error(y_test_inversed, y_pred_inversed)  # Mean Absolute Percentage Error\n",
    "\n",
    "    # Store the results in the dictionary\n",
    "    evaluation = {\n",
    "        'rmse': round(rmse, 4),\n",
    "        'mae': round(mae, 4),\n",
    "        'mape': round(mape, 4),\n",
    "    }\n",
    "\n",
    "    return evaluation\n",
    "\n",
    "\n",
    "\n",
    "def workflow_results(excel_path, list_sheet_name, list_model, test_size, forecast_horizon, list_lags_covariates, list_lags_target):\n",
    "    # Load the Excel file\n",
    "    xls = pd.ExcelFile(excel_path)\n",
    "\n",
    "    # Dictionary to store the results by sheet_name, by target variable and then group them by models\n",
    "    dict_results = {}\n",
    "\n",
    "    # Iterate through each sheet name to process bronze to silver and silver to gold transformations\n",
    "    for sheet_name in list_sheet_name:\n",
    "        print(\"sheet_name:\", sheet_name)\n",
    "\n",
    "        # Initialize the dictionary to store results for the current sheet_name\n",
    "        if sheet_name not in dict_results:\n",
    "            dict_results[sheet_name] = {}\n",
    "\n",
    "        # Read the sheet into a dataframe\n",
    "        bronze_df = pd.read_excel(xls, sheet_name=sheet_name)\n",
    "\n",
    "        # Apply the bronze_to_silver transformation\n",
    "        silver_df = bronze_to_silver(df=bronze_df)\n",
    "\n",
    "        # Apply the silver_to_gold transformation\n",
    "        gold_df = silver_to_gold(df=silver_df)\n",
    "\n",
    "        list_target_variables = []\n",
    "        for column in gold_df.columns:\n",
    "            if column.startswith(\"HICP\"):\n",
    "                list_target_variables.append(column)\n",
    "\n",
    "        for target_variable in list_target_variables:\n",
    "            print(\"    target:\", target_variable)\n",
    "\n",
    "            # Initialize the dictionary to store results for the current target_variable in the target_variable sheet_name\n",
    "            if target_variable not in dict_results[sheet_name]:\n",
    "                dict_results[sheet_name][target_variable] = {}\n",
    "\n",
    "            X_train, X_test, y_train, y_test, scaler_X_fitted, scaler_y_fitted = gold_to_model_data(\n",
    "                df=gold_df, target_variable=target_variable, list_target_variables=list_target_variables, test_size=test_size,\n",
    "                list_lags_covariates=list_lags_covariates, list_lags_target=list_lags_target\n",
    "            )\n",
    "\n",
    "            # Apply each model function to the current target_variable from the current sheet_name's data\n",
    "            for model_func in list_model:\n",
    "                print(\"        model:\", model_func.__name__)\n",
    "\n",
    "                # Initialize the dictionary to store results for the current target_variable in the target_variable sheet_name\n",
    "                if model_func.__name__ not in dict_results[sheet_name][target_variable]:\n",
    "                    dict_results[sheet_name][target_variable][model_func.__name__] = {}\n",
    "\n",
    "                # Apply the predictive model to obtain its evaluation\n",
    "                model_evaluation = model_data_to_model_evaluation(\n",
    "                    target_variable=target_variable, model_func=model_func,\n",
    "                    X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test,\n",
    "                    scaler_X_fitted=scaler_X_fitted, scaler_y_fitted=scaler_y_fitted,\n",
    "                    forecast_horizon=forecast_horizon, list_lags_target=list_lags_target\n",
    "                )\n",
    "\n",
    "                # Store the model evaluation metrics under the corresponding model name\n",
    "                dict_results[sheet_name][target_variable][model_func.__name__] = model_evaluation\n",
    "        print()\n",
    "    return dict_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiate workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Path to the Excel file\n",
    "folder_path = \"C:/Users/put/your/folder/path/that/points/to/file\"\n",
    "excel_file = \"MFW_VascoCostaLeal_EUED_dataset.xlsx\"\n",
    "excel_path = f\"{folder_path}/{excel_file}\"\n",
    "\n",
    "# Setting variables\n",
    "list_sheet_name = [\"EU27\", \"EA20\", \"Germany\", \"UK\"]\n",
    "list_model = [\n",
    "    random_walk, autoregressive,\n",
    "    lasso, ridge, elastic_net,\n",
    "    bagging, gradient_boosting, random_forest,\n",
    "]\n",
    "\n",
    "test_size = 12\n",
    "forecast_horizon = 12\n",
    "list_lags_covariates = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "list_lags_target = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "\n",
    "\n",
    "results = workflow_results(\n",
    "    excel_path=excel_path, list_sheet_name=list_sheet_name, list_model=list_model,\n",
    "    test_size=test_size, forecast_horizon=forecast_horizon, \n",
    "    list_lags_covariates=list_lags_covariates, list_lags_target=list_lags_target\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(results)\n",
    "# print(results[\"EU27\"])\n",
    "# print(results[\"EU27\"][\"HICP_All_mor_NA\"])\n",
    "# print(results[\"EU27\"][\"HICP_All_mor_NA\"][\"linear_regression\"])\n",
    "# print(results[\"EU27\"][\"HICP_All_mor_NA\"][\"linear_regression\"][\"rmse\"])\n",
    "\n",
    "\n",
    "# Flattening the results into a tabular format\n",
    "table_data = []\n",
    "\n",
    "for region, targets in results.items():\n",
    "    for target_variable, models in targets.items():\n",
    "        for model_name, metrics in models.items():\n",
    "            table_data.append([region, target_variable, model_name, metrics['rmse'], metrics['mae']])\n",
    "\n",
    "df = pd.DataFrame(table_data, columns=['Region', 'Target Variable', 'Model', 'RMSE', 'MAE'])\n",
    "display(df)\n",
    "\n",
    "\n",
    "folder_path = \"C:/Users/vcostlea/OneDrive - NTT DATA EMEAL/Desktop/MFW/\"\n",
    "results_file = \"results\"\n",
    "results_path = f\"{folder_path}/Results/{results_file}\"\n",
    "index=False\n",
    "header=True\n",
    "\n",
    "df.to_excel(f\"{results_path}.xlsx\", index=index, header=header)\n",
    "df.to_csv(f\"{results_path}.csv\", index=index, header=header)\n",
    "\n",
    "print(f\"File saved\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
